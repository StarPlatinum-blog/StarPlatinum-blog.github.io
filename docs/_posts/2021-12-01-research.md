---
layout: post
title: Research
date: 21-12-01 08:11:04 +0800
categories: nots
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# 文献阅读

目标跟踪、3D目标检测、多模态3维目标检测

- [1. Object-Level Fusion for Surround Environment Perception in Automated Driving Applications](#paper1)
- [2. Robot Localization I: Recursive Bayesian Estimation](#paper2)
- [3. Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving](#paper3)
- [4. Frustum PointNets for 3D Object Detection from RGB-D Data](#paper4)
- [5. EPNet++: Cascade Bi-directional Fusion for Multi-Modal 3D Object Detection](#paper5)
- [6. IPOD: Intensive Point-based Object Detector for Point Cloud](#paper6)

3D 检测算法分类：

1. 基于点云采样得到的Voxel，利用3D CNN进行检测:
   - Fast point r-cnn:Y. Chen, S. Liu, X. Shen, and J. Jia, “Fast point r-cnn,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.
   - Voxelnet: http://dx.doi.org/10.1109/CVPR.2018.00472
   - Voxnet: Voxnet: A 3d convolutional neural network for real-time object recognition, 2015
   - Octnet: Learning deep 3d representations at high resolutions,
2. 基于点云BEV视图的2D投影，应用2D CNN进行检测：
   - Pixor: Pixor: Real-time 3d object detection from point clouds, 2018
   - Multi-view 3d object detection network for autonomous driving, 2017
3. 基于2D检测生成的3D视锥，对点进行检测
   - F-ConvNet: Frustum ConvNet: Sliding Frustums to
     Aggregate Local Point-Wise Features for Amodal 3D Object Detection, 2019
   - F-PointNet: Frustum PointNets for 3D Object Detection from RGB-D Data, 2018
   - 3d object detection using scale invariant and feature reweighting networks, 2019
   - Roarnet: A robust 3d object detection based on region approximation refinement, 2019
4. 

## 1. Object-Level Fusion for Surround Environment Perception in Automated Driving Applications

马尔科夫假设(Markov assumption)：下一个词的出现仅依赖于它前面的一个或几个词。

Information Matrix fusion：最优结果

Dempster-Shafer evidence theory：目标存在性融合

### 1.1 关联

Planar rectangle object model。

Three steps to do an association:

1.   Feature selection
2.   state vector association
3.   geometrical association

量化描述关联质量的值（使用了马氏距离Mahalanobis distance）：



>   马氏距离：一种距离的度量，可以看做是欧氏距离的一种修正，修正了欧氏距离中各个维度尺度不一致且相关的问题，同欧氏距离曼哈顿距离、汉明距离等一样，用于评定数据之间的相似度指标。
>
>   单个数据点的马氏距离：
>   $$
>   D_{M}(\bold{x})=\sqrt{(\bold{x}-\bold{\mu})^{'}\mathbf{S^{-1}}(\bold{x}-\mu)}
>   $$
>
>   两个数据点（x, y）之间的马氏距离：
>   $$
>   D_{M}(\bold{x},\bold{y})=\sqrt{(\bold{x-y})^{'}\mathbf{S}^{-1}(\bold{x-y})}
>   $$
>   $$\mathbf{S}$$代表多维随机变量的协方差矩阵，$$\mu$$代表样本均值；如果协方差是单位阵，即各维度独立同分布，则马氏距离变为欧式距离。

innovation（测量残差）：实际传感器测量值和传感器预测值之间的差。



## 2. Robot Localization I: Recursive Bayesian Estimation

https://www.sabinasz.net/robot-localization-recursive-bayesian-estimation/



## 3. Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving
> WACV 2022

Abstract：

> We present an end-to-end method for object detection and trajectory prediction utilizing multi-view representations of LiDAR returns and camera images. In this work, we recognize the strengths and weaknesses of different view representations, and we propose an efficient and generic fusing method that aggregates benefits from all views. Our model builds on a state-of-the-art Bird’s-Eye View (BEV) network that fuses voxelized features from a sequence of historical LiDAR data as well as rasterized high-definition map to perform detection and prediction tasks. We extend this model with additional LiDAR Range-View (RV) features that use the raw LiDAR information in its native, nonquantized representation. The RV feature map is projected into BEV and fused with the BEV features computed from LiDAR and high-definition map. The fused features are then further processed to output the final detections and trajectories, within a single end-to-end trainable network. In addition, the RV fusion of LiDAR and camera is performed in a straightforward and computationally efficient manner using this framework. The proposed multi-view fusion approach improves the state-of-the-art on proprietary largescale real-world data collected by a fleet of self-driving vehicles, as well as on the public nuScenes data set with minimal increases on the computational cost.

我们提出了一种利用 LiDAR 返回和相机图像的**多视图表示（multi-view representation）**来进行**目标检测**和**轨迹预测**的端到端方法。在这项工作中，我们认识到不同视图表示的优缺点，并提出了一种有效且通用的融合方法，可以聚合所有视图的好处。我们的模型建立在SOTA Bird’s-Eye View (BEV ) network之上，该网络融合了来自一系列历史 LiDAR 数据的**体素化特征**以及**光栅化的高精度地图**，以执行*检测*和*预测*任务。我们使用 **LiDAR Range-View (RV) 功能**扩展此模型，这些功能在其原生、非量化表示中使用原始 LiDAR 信息。 RV 特征图投影到 BEV 中，并与从 LiDAR 和高清地图计算的 BEV 特征融合。然后在单个端到端可训练网络中进一步处理融合特征以输出最终检测和轨迹。此外，LiDAR 和摄像头的 RV 融合是使用该框架以直接且计算高效的方式执行的。所提出的多视图融合方法提高了自动驾驶车队收集的专有大规模真实世界数据的最新技术水平，以及公共 nuScenes 数据集的计算成本增加最小。

1. 什么是多视图表示？是指不同视角的摄像头和激光雷达数据吗？
2. 什么是光栅化的高精度地图？
3. 什么是LiDAR Range-View？
4. RV + Lidar 体素化特征 + 高精地图光栅化特征，Feature级的融合
5. 

### 3.1 视图（View）

关于视图的解释：[巫婆塔里的工程师——知乎](https://zhuanlan.zhihu.com/p/406674156)

视图（View），也就是点云数据的不同表示方法，常用的包括

- Bird's Eye View （BEV）
- Point View（PV）
- Range View （RV）

![]({{ site.url }}/imgs/research/3/views.jpg)

### 3.2 LiDAR检测算法分类

点云目标检测算法在很多综述中粗略分为四类：

- Multi-View方法
- Voxel方法（BEV）：VoxelNet（3D卷积，计算量太大）
  - SECOND：稀疏卷积，避免了空白区域的无效计算，降低了显存消耗
  - PIXOR：手工设计，将3D的Voxel压缩到2D的Pixel，避免了3D卷积，但损失了高度信息
  - PointPillar：将网格内的点特征堆叠在一起
- Point方法（PV）：PointNet++
- Voxel结合Point方法

基于部署的分类：

- Pillar-based：PointPillars
  - 难点：前处理
- Voxel-based：SECOND
  - 难度：前处理，Spconv
- Point-based：PointRCNN
  - 慢，SA，FP
  - 快，3DSSD

基于特征提取器：

- Voxel-based：点云投影到体素，用3维卷积或者稀疏卷积提取特征；
- Pillar-based：把点云特征投影到不要z轴信息的pillar上，用2D卷积进行特征提取；
- Range-image-based（RSN）：激光最原始，最紧致的表示；
- Point-based（Point RCNN）

基于detection head：

- Anchor-based（RCNN family）
- Center-based：用物体的中心代替物体

雷达Range View数据生成的方法：类似于针孔摄像机模型中相机坐标到像素坐标的投影。

![]({{ site.url }}/imgs/research/3/rv-generate.jpg)

基于Range View的算法：

- RangeDet：中科院
- Range Sparse Net：Waymo

## 4. Frustum PointNets for 3D Object Detection from RGB-D Data

> CVPR 2018

Abstract:

>In this work, we study 3D object detection from RGBD data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.

在这项工作中，我们研究了室内和室外场景中 RGBD 数据的 3D 对象检测。 虽然以前的方法专注于图像或 3D 体素，通常会掩盖自然 3D 模式和 3D 数据的不变性，但我们通过**popping up RGB-D 扫描**直接对原始点云进行操作。 然而，这种方法的一个关键挑战是如何有效地定位大规模场景点云中的**对象**（region proposal）。 我们的方法不是仅仅依靠 3D  proposal，而是利用成熟的 2D 对象检测器和先进的 3D 深度学习进行对象定位，即使是小对象也能实现**高效率和高召回率**。 受益于直接在原始点云中学习，我们的方法即使在强遮挡或非常稀疏的点下也能够精确估计 3D 边界框。 在 KITTI 和 SUN RGB-D 3D 检测基准上进行了评估，我们的方法在具有实时能力的同时显着优于现有技术。

问题：

- 依赖2D检测器

## 5. EPNet++: Cascade Bi-directional Fusion for Multi-Modal 3D Object Detection



## 6. IPOD: Intensive Point-based Object Detector for Point Cloud

Abstract

> We present a novel 3D object detection framework, named IPOD, based on raw point cloud. It seeds object proposal for each point, which is the basic element. This paradigm provides us with high recall and high fidelity of information, leading to a suitable way to process point cloud data. We design an end-to-end trainable architecture, where features of all points within a proposal are extracted from the backbone network and achieve a proposal feature for final bounding inference. These features with both context information and precise point cloud coordinates yield improved performance. We conduct experiments on KITTI dataset, evaluating our performance in terms of 3D object detection, Bird’s Eye View (BEV) detection and 2D object detection. Our method accomplishes new state-of-the-art , showing great advantage on the hard set.

我们提出了一种基于原始点云的新型 3D 对象检测框架，名为IPOD。 它为每个点播种对象建议，这是基本元素。 这种范式为我们提供了高召回率和高保真度的信息，从而为处理点云数据提供了一种合适的方式。 我们设计了一个端到端的可训练架构，其中提案中所有点的特征都从骨干网络中提取出来，并为最终的边界推断实现了提案特征。 这些具有上下文信息和精确点云坐标的特征可以提高性能。 我们在 KITTI 数据集上进行实验，评估我们在 3D 对象检测、鸟瞰 (BEV) 检测和 2D 对象检测方面的性能。 我们的方法实现了新的 state-of-the-art，在硬集上显示出很大的优势。

雷达点云的优劣：

- 提供了有精确深度和相对位置的目标空间和结构信息；
- 点云是较为稀疏的、无序的、分布不均匀；



## 7. PillarNet: High-Performance Pillar-based 3D Object Detection





## 8. TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection

> Motivation: 对标定误差更加Robust

硬关联——软关联：除了位置关系以外，还考虑内容的关系；

利用Transformer的Attention机制，实现*soft association*

- 通过Object Query。。。
- Attention fusion

问题：

- Query初始化只用了激光的BEV特征，如果这部分没有图像中一些模板的信息，也会影响后面的融合，检测受限于Query Initialization
  - Image-Guided Query Initializtion：加入图像指导（[Tesla BEV Transformer](https://towardsdatascience.com/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944)），Cross-Attention

### 8.1 多模态检测算法分类

- In cascade（F-PointNet）：由一个模态进行检测后，在根据检测信息在另一个模特进行检测；
  - 多个模态的信息没有很好的交互
- Proposal level：在各个模态的ROI进行特征提取，然后在进行融合。
  - ROI区域可能存在噪声
- Point level（PointPainting，PointAugmenting  CVPR2021，EPNet）：基于相机雷达外参，找到投影关系后做检测
  - PointPainting
    - 硬关联（相机和图像之间的关联点是固定的），在激光雷达点较少时信息较少，浪费图像特征；
    - 依赖投影精度
    - 在图像较差时（低光照），也会利用图像特征，导致算法Robustness下降

### Future work

1. Extending TransFusion to an online 3D tracking
2. Camera-only 3D detection
   - Tesla AI day: cross-attention
   - 通过几何特征+深度估计，估计深度的分布^[1]^，然后做三维模板检测^[2]^
3. Range-image representation for efficient 3D detection
   - 不会丢掉点信息
   - 图森：RangeDet
   - Waymo：RSN
4. Temporal information for robust 3D detection
   - 3D-MAN



## 9. RangeDet:In Defense of Range View for LiDAR-based 3D Object Detection



## 10. RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection



## 11. CLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection



## 12. MV3D

> Multi-View 3D Object Detection Network for Autonomous Driving, 2019

- 3D BBox + feature fusion

将点云投影到BEV视图，利用2D卷积提取特征，并生成3D BBox。然后将3D BBox投影到点云的前视图以及图像上，并将相应的特征进行融合。

存在的问题：

1. 将点云转换到BEV视图失去了Z轴空间信息；
2. 将不同尺寸的特征向量通过裁剪、调整大小来聚合在一起，可能会摧毁传感器的特征结构；



## Misc

神经网络算法跑的慢的主要原因：矩阵乘法，而矩阵乘法慢的原因是**访存**。



## References

1. [Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D](https://arxiv.org/abs/2008.05711)
2. [BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View](https://arxiv.org/abs/2112.11790)



























