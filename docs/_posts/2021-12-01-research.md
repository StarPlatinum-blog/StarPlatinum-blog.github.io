---
layout: post
title: Research
date: 21-12-01 08:11:04 +0800
categories: nots
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
# 文献阅读

## 1. Object-Level Fusion for Surround Environment Perception in Automated Driving Applications

马尔科夫假设(Markov assumption)：下一个词的出现仅依赖于它前面的一个或几个词。

Information Matrix fusion：最优结果

Dempster-Shafer evidence theory：目标存在性融合

### 1.1 关联

Planar rectangle object model。

Three steps to do an association:

1.   Feature selection
2.   state vector association
3.   geometrical association

量化描述关联质量的值（使用了马氏距离Mahalanobis distance）：



>   马氏距离：一种距离的度量，可以看做是欧氏距离的一种修正，修正了欧氏距离中各个维度尺度不一致且相关的问题，同欧氏距离曼哈顿距离、汉明距离等一样，用于评定数据之间的相似度指标。
>
>   单个数据点的马氏距离：
>   $$
>   D_{M}(\bold{x})=\sqrt{(\bold{x}-\bold{\mu})^{'}\mathbf{S^{-1}}(\bold{x}-\mu)}
>   $$
>
>   两个数据点（x, y）之间的马氏距离：
>   $$
>   D_{M}(\bold{x},\bold{y})=\sqrt{(\bold{x-y})^{'}\mathbf{S}^{-1}(\bold{x-y})}
>   $$
>   $$\mathbf{S}$$代表多维随机变量的协方差矩阵，$$\mu$$代表样本均值；如果协方差是单位阵，即各维度独立同分布，则马氏距离变为欧式距离。

innovation（测量残差）：实际传感器测量值和传感器预测值之间的差。



## 2. Robot Localization I: Recursive Bayesian Estimation

https://www.sabinasz.net/robot-localization-recursive-bayesian-estimation/



## 3. Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving
> WACV 2022

Abstract：

> We present an end-to-end method for object detection and trajectory prediction utilizing multi-view representations of LiDAR returns and camera images. In this work, we recognize the strengths and weaknesses of different view representations, and we propose an efficient and generic fusing method that aggregates benefits from all views. Our model builds on a state-of-the-art Bird’s-Eye View (BEV) network that fuses voxelized features from a sequence of historical LiDAR data as well as rasterized high-definition map to perform detection and prediction tasks. We extend this model with additional LiDAR Range-View (RV) features that use the raw LiDAR information in its native, nonquantized representation. The RV feature map is projected into BEV and fused with the BEV features computed from LiDAR and high-definition map. The fused features are then further processed to output the final detections and trajectories, within a single end-to-end trainable network. In addition, the RV fusion of LiDAR and camera is performed in a straightforward and computationally efficient manner using this framework. The proposed multi-view fusion approach improves the state-of-the-art on proprietary largescale real-world data collected by a fleet of self-driving vehicles, as well as on the public nuScenes data set with minimal increases on the computational cost.

我们提出了一种利用 LiDAR 返回和相机图像的**多视图表示（multi-view representation）**来进行**目标检测**和**轨迹预测**的端到端方法。在这项工作中，我们认识到不同视图表示的优缺点，并提出了一种有效且通用的融合方法，可以聚合所有视图的好处。我们的模型建立在SOTA Bird’s-Eye View (BEV ) network之上，该网络融合了来自一系列历史 LiDAR 数据的**体素化特征**以及**光栅化的高精度地图**，以执行*检测*和*预测*任务。我们使用 **LiDAR Range-View (RV) 功能**扩展此模型，这些功能在其原生、非量化表示中使用原始 LiDAR 信息。 RV 特征图投影到 BEV 中，并与从 LiDAR 和高清地图计算的 BEV 特征融合。然后在单个端到端可训练网络中进一步处理融合特征以输出最终检测和轨迹。此外，LiDAR 和摄像头的 RV 融合是使用该框架以直接且计算高效的方式执行的。所提出的多视图融合方法提高了自动驾驶车队收集的专有大规模真实世界数据的最新技术水平，以及公共 nuScenes 数据集的计算成本增加最小。

1. 什么是多视图表示？是指不同视角的摄像头和激光雷达数据吗？
2. 什么是光栅化的高精度地图？
3. 什么是LiDAR Range-View？
4. RV + Lidar 体素化特征 + 高精地图光栅化特征，Feature级的融合
5. 

### 3.1 视图（View）

关于视图的解释：[巫婆塔里的工程师——知乎](https://zhuanlan.zhihu.com/p/406674156)

视图（View），也就是点云数据的不同表示方法，常用的包括

- Bird's Eye View （BEV）
- Point View（PV）
- Range View （RV）

![](../imgs/research/3/views.jpg)

LiDAR点云目标检测算法在很多综述中粗略分为四类：

- Multi-View方法
- Voxel方法（BEV）：VoxelNet（3D卷积，计算量太大）
  - SECOND：稀疏卷积，避免了空白区域的无效计算，降低了显存消耗
  - PIXOR：手工设计，将3D的Voxel压缩到2D的Pixel，避免了3D卷积，但损失了高度信息
  - PointPillar：将网格内的点特征堆叠在一起
- Point方法（PV）：PointNet++
- Voxel结合Point方法

雷达Range View数据生成的方法：类似于针孔摄像机模型中相机坐标到像素坐标的投影。

![](../imgs/research/3/rv-generate.jpg)

基于Range View的算法：

- RangeDet：中科院
- Range Sparse Net：Waymo

## 4. Frustum PointNets for 3D Object Detection from RGB-D Data

> CVPR 2018

Abstract:

>In this work, we study 3D object detection from RGBD data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.

在这项工作中，我们研究了室内和室外场景中 RGBD 数据的 3D 对象检测。 虽然以前的方法专注于图像或 3D 体素，通常会掩盖自然 3D 模式和 3D 数据的不变性，但我们通过**popping up RGB-D 扫描**直接对原始点云进行操作。 然而，这种方法的一个关键挑战是如何有效地定位大规模场景点云中的**对象**（region proposal）。 我们的方法不是仅仅依靠 3D  proposal，而是利用成熟的 2D 对象检测器和先进的 3D 深度学习进行对象定位，即使是小对象也能实现**高效率和高召回率**。 受益于直接在原始点云中学习，我们的方法即使在强遮挡或非常稀疏的点下也能够精确估计 3D 边界框。 在 KITTI 和 SUN RGB-D 3D 检测基准上进行了评估，我们的方法在具有实时能力的同时显着优于现有技术。

## 5. EPNet++: Cascade Bi-directional Fusion for Multi-Modal 3D Object Detection

## 6. IPOD: Intensive Point-based Object Detector for Point Cloud

Abstract

> We present a novel 3D object detection framework, named IPOD, based on raw point cloud. It seeds object proposal for each point, which is the basic element. This paradigm provides us with high recall and high fidelity of information, leading to a suitable way to process point cloud data. We design an end-to-end trainable architecture, where features of all points within a proposal are extracted from the backbone network and achieve a proposal feature for final bounding inference. These features with both context information and precise point cloud coordinates yield improved performance. We conduct experiments on KITTI dataset, evaluating our performance in terms of 3D object detection, Bird’s Eye View (BEV) detection and 2D object detection. Our method accomplishes new state-of-the-art , showing great advantage on the hard set.

我们提出了一种基于原始点云的新型 3D 对象检测框架，名为IPOD。 它为每个点播种对象建议，这是基本元素。 这种范式为我们提供了高召回率和高保真度的信息，从而为处理点云数据提供了一种合适的方式。 我们设计了一个端到端的可训练架构，其中提案中所有点的特征都从骨干网络中提取出来，并为最终的边界推断实现了提案特征。 这些具有上下文信息和精确点云坐标的特征可以提高性能。 我们在 KITTI 数据集上进行实验，评估我们在 3D 对象检测、鸟瞰 (BEV) 检测和 2D 对象检测方面的性能。 我们的方法实现了新的 state-of-the-art，在硬集上显示出很大的优势。
