---
layout:	post
title:	Baidu ApolloAuto Apollo platform
date:   2022-03-23 08:12:00 +0800
categories: note
---

[TOC]

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

## 百度Apollo源码阅读

### 1. 目标融合部分

#### 1.1 融合逻辑

融合组件入口：`modules/perception/onboard/component/fusion_component.h`

##### 融合方式：`FusionComponent`

- 后融合

- 感知目标(传感器sensor) + 全局目标(场景scene)：接受并存储感知目标数据，与保存时间最近的所有感知目标一起与全局目标进行融合；
- 事件驱动：通过协程对Channel进行事件轮询，对接收到的感知数据立即进行融合；

##### 融合流程：`ProbabilisticFusion::Fuse`

1. 保存感知帧，并判断是否是主传感器(velodyne128)，如果不是，直接返回；

2. 根据当前感知帧时间戳取得最近的所有感知帧（上次取出感知帧的时间 ———— 本帧时间）；

3. 对取出的感知帧进行融合；
   1. 对前景目标进行融合`FuseForegroundTrack`；
      1. 感知帧前景目标与场景帧前景轨迹匹配（HM）`matcher_->Associate(options, frame, scenes_, &association_result);`；
      
      2. 更新匹配到的前景轨迹
      
         `this->UpdateAssignedTracks(frame, assignments);`；
      
         ```c++
         size_t track_ind = assignments[i].first;
         size_t obj_ind = assignments[i].second;
         trackers_[track_ind]->UpdateWithMeasurement(
                 options, frame->GetForegroundObjects()[obj_ind], frame->GetTimestamp());
         ```
      
      3. 更新未匹配的前景轨迹
      
         `this->UpdateUnassignedTracks(frame, unassigned_track_inds);`；
      
         ```c++
         size_t track_ind = unassigned_track_inds[i];
             trackers_[track_ind]->UpdateWithoutMeasurement(
                 options, sensor_id, frame->GetTimestamp(), frame->GetTimestamp());
         ```
      
      4. 使用未匹配的传感器目标创建新的轨迹
      
         `this->CreateNewTracks(frame, unassigned_obj_inds);`；
      
         1. 判断当前传感器是否被禁止产生新轨迹；
      
            `FusionParams::prohibition_sensors`
      
         2. 将传感器帧的前景目标作为轨迹加入场景中；
         
            `scenes_->AddForegroundTrack(track);`
         
         3. 使用传感器帧的前景目标初始化一个概率跟踪器`PbfTracker`，并加入跟踪器列表；（这个跟踪器将在前面两步中用于更新轨迹）
         
            `tracker->Init(track, frame->GetForegroundObjects()[obj_ind]);`
      
   2. 对后景目标进行融合`FusebackgroundTrack`；
   
      1. 感知帧后景目标与场景后景匹配（id匹配）；
   
      2. 更新匹配到的后景轨迹；(注意这里用的函数跟上面的不同，不是PbfTracker)
   
         ```c++
         background_tracks[track_ind]->UpdateWithSensorObject(
             frame_objs[obj_ind]);
         ```
   
      3. 更新未匹配到的后景轨迹；
   
         ```c++
         background_tracks[i]->UpdateWithoutSensorObject(sensor_id,
         frame->GetTimestamp());
         ```
   
      4. 创建新后景轨迹；
   
         1. 直接将未匹配到的目标加入场景的后景轨迹；
   
   3. 清除丢失的目标`RemoveLostTrack`；
   
      1. 遍历轨迹，通过`IsAlive`函数检查目标活动性，同时删除轨迹和跟踪器；
   
         目标活动性更新：
   
         1. 轨迹初始化时设置为true；
         2. 后景目标存在匹配时，设置为true；
         3. 后景目标不存在匹配时，当后景目标的激光雷达、毫米波雷达和摄像头目标队列全空时，设置为false；
   
4. 输出融合后的帧。

##### 概率跟踪器：`PbfTracker`

包含对目标四种属性的融合：

1. 类型：DS证据理论；
2. 运动状态：卡尔曼滤波；
3. 存在性：DS证据理论
4. 形状：概率融合。

跟踪流程：

1. 存在性融合；
2. 运动状态融合；
3. 形状融合；
4. 类型融合；
5. 更新轨迹。



#### 1.2 存在性融合

##### 1.2.1 DS证据理论

[DS证据理论](https://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory)（DST: Dempster-Shafer theory）：Dempster-Shafer 理论是贝叶斯主观概率理论的推广。 信任函数将一个问题的信任度建立在相关问题的主观概率上。 置信度本身可能具有也可能不具有概率的数学特性； 它们有多少不同取决于这两个问题的相关程度。

- 主要特点：满足比贝叶斯概率理论更弱的条件；具有直接表达“不确定”和“不知道”的能力。

> Dempster A P. Upper and lower probabilities induced by a multivalued mapping[M]. Annals of Mathematical Statistics, 1967 [首次提出]
>
> Shafer, G. A Mathematical Theory of Evidence. Princeton University Press, 1976. [第一本证据理论专著]
>
> Zadeh, L. A. Review of Shafer's a mathematical theory of evidence. AI Magazine, 1984. [对证据理论的质疑]

1. 假设空间/识别框架（fod: frame of discernment）：一个包含了若干个假设的集合；

2. 基本概率分配（BPA：Basic Probability Assignment）：在识别框架 $\Theta$ 上，假设$P(\Theta)$表示$\Theta$中所有可能的子集构成的集合，BPA表示为一个$P(\Theta) \to [0,1]$的函数，称为mass函数，并满足以下等式。使得$m(A)>0$的$A$称为焦元（Focal elements）

   $$
   m(\emptyset)=0; \displaystyle \sum_{A \subseteq H}{m(A)} = 1
   $$

3. 信任函数/信度函数（Belief function）：在识别框架$\Theta$上，基于BPA的信任函数定义为：一个假设全部子集的概率分配之和：

   $$
   Bel(A)=\sum\limits_{B \subseteq A}m(B)
   $$

4. 似然函数（Plausibility function）：在识别框架$\Theta$上，基于BPA的似然函数定义为：一个假设的所有交集的概率分配之和：

   $$
   Pl(A) = \sum\limits_{B \cap A \neq \emptyset}m(B)
   $$
   
5. 信任区间：对假设A根据BPA分别计算出关于该假设的信任函数和似然函数组成信任区间，以表示对假设A的确认程度：$[Bel(A),Pl(A)]$

5. 不确定度：$Pl(A) - Bel(A)$

6. 证据合成公式：在识别框架$\Theta$上，对于$\forall A \subseteq \Theta$，两个BPA的合成规则为：
   $$
   m_1(A) \oplus m_2(A) = \frac{1}{K} \sum \limits_{B \cap C = A} m_1(B) \times m_2(C)
   \\
   K = \sum \limits_{B \cap C \neq \emptyset} m_1(B) \times m_2(C)
   $$
   

##### 1.2.2 基于DS证据理论的存在性融合

> modules/perception/fusion/lib/data_fusion/existence_fusion/dst_existence_fusion/dst_existence_fusion.h

（1）假设空间/识别框架定义：

- 存在性：`EXIST` `NEXIST` `EXISTUNKNOWN = {EXIST, NEXIST}`

- 关注目标是否在摄像头判断区域（TOIC: target of interest in camera judgement）：`TOIC` `NTOIC` `TOICUNKNOWN = {TOIC, NTOIC}`

（2）参数配置：

- 摄像头最大可用范围，用于更新TOIC：

  ```c++
  {"camera_smartereye", 110},
  {"camera_front_obstacle", 110},
  {"camera_front_narrow", 150},
  {"front_6mm", 110},
  ```

- 轨迹与目标最大匹配距离（max\_match\_dist），用于计算关联概率：4.0

（3）关联概率`association_prob`：

> `match_distance`为用于融合的传感器帧与其匹配的场景中保存的轨迹之间的距离。

- 摄像头：1.0
- 其他传感器：$1-\frac{match\_distance}{match\_distance_{max}} $

（4）传感器可靠性`sensor_reliablity`：

- 首先检查目标类型，如果类型未知，则按以下设置：

  - 激光雷达：0.54

  - 摄像头：0.48

  - 其他传感器：0.6（超声波雷达、毫米波雷达）

- 如果类型已知，则按以下设置：

  - 激光雷达：0.9
  - 摄像头：0.8
  - 其他传感器：0.6

（5）存在性衰减`decay`计算：

- 毫米波雷达：由速度计算衰减`ComputeFeatureInfluence(measurement);`

  1. 计算速度因子：$factor_v = v > 4.0\ ?\ sigmoid(v) : 0$
  2. 由速度计算衰减：$decay = factor_v \times existance\_confidence_{obj}$

- 其他传感器：由到传感器的距离计算衰减`ComputeDistDecay`

  - 目标和传感器间距离>60m：0.8

  - 目标和传感器间距离<=60m：1.0

（6）目标存在概率计算：
$$
exist\_prob=sensor\_reliability \times decay
$$

**融合流程**：

1. 获取参数配置（最大匹配距离）；

2. 计算匹配概率、存在概率；

3. 设置假设空间BPA：
   $$
   sensor\_bpa =
   \left\{
   \begin{align}
   
   m(EXIST)&=exist\_prob
   \\
   m(NEXIST)&=0
   \\
   m(EXISTUNKNOWN)&=1-exist\_prob
   \end{align}
   \right.
   $$
   
4. 与存储的上一帧BPA利用DST计算证据合成：
   $$
   \hat{bpa_{fused}} = \hat{bpa_{fused}^{-}} \oplus (sensor\_bpa \times association\_prob)
   $$

5. （Optional）如果感知目标来自摄像头，则根据最新的激光雷达目标（没有则用最近的毫米波目标）在摄像头视野中的比例，利用DST更新TOIC；

   `UpdateToicWithCameraMeasurement`

6. 根据证据合成计算融合目标的存在概率和TOIC概率。`UpdateExistenceState`

   1. 计算TOIC概率：
   2. 计算存在概率：`GetExistenceProbability()`
   



#### 1.3 类型融合

##### 1.3.1 基于DS证据理论的类型融合

> modules/perception/fusion/lib/data_fusion/type_fusion/dst_type_fusion/dst_type_fusion.h

（1）假设空间/识别框架定义：（目标类型定义）

- 基础类型：
  - `Pedestrian`
  - `Bicycle`
  - `Vehicle`
  - `Others_Movable`
  - `Others_Unmovable`
- 集合类型：
  - `Others = {Others_Movable, Others_Unmovable}`
  - `Unknown = {Pedestrian, Bicycle, Vehicle, Others}`

（2）参数配置：

- 摄像头最大有效距离：

  ```
  {"camera_smartereye", 110},
  {"camera_front_obstacle", 110},
  {"front_6mm", 110},
  {"camera_front_narrow", 150},
  ```

- 传感器可靠性`Reliability`：未列出的传感器不进行类型融合

  ```
  {"velodyne16", 0.5}, {"velodyne64", 0.5}, {"velodyne_64", 0.5},
  {"velodyne128", 0.5},         {"camera_smartereye", 0.95},
  {"front_6mm", 0.95},          {"camera_front_obstacle", 0.95},
  {"camera_front_narrow", 0.5},
  ```

- 传感器对`Unknown`类型识别的可靠性`Reliability for unknown`：

  ```
  {"velodyne16", 0.5}, {"velodyne64", 0.5}, {"velodyne_64", 0.5},
  {"velodyne128", 0.5},         {"camera_smartereye", 0.2},
  {"front_6mm", 0.2},           {"camera_front_obstacle", 0.2},
  {"camera_front_narrow", 0.2},
  ```

**融合流程**：

1. 获取传感器可靠性；

2. 根据传感器类型识别概率配置感知类型BPA；

3. 与存储的上一帧BPA计算证据合成：
   $$
   \hat{bpa_{fused}} = \hat{bpa_{fused}^{-}} \oplus (sensor\_bpa \times Reliability)
   $$

4. （Optional）如果感知结果来自摄像头，设置融合目标的次类型(sub\_type)为感知结果的次类型；

5. 更新计算结果；

   1. 从上面计算得到的BPA中获取mass最大的类型`t_max`；
   2. 更新目标类型为`t_max`；
   2. 用BPA更新各类型概率（直接赋值）；



- 次类型与类型对应关系：

![]({{ site.url }}/imgs/apollo/perception_subtype_2_type.png)



#### 1.4 形状融合

形状融合包括目标的形状以及中心点进行融合，都是直接由感知数据更新跟踪数据。

- 尺寸：长宽高；
- 方向：三维向量；
- yaw；
- 目标多边形；
- 中心点；
- 锚点；

根据感知数据来源的传感器，按照以下规则更新：

数据来自：

1. 激光雷达：更新全部属性；
2. 毫米波雷达：
   1. 如果存在最近的激光雷达数据帧，则不更新；
   2. 如果不存在最近的激光雷达帧，则：
      1. 如果存在最近的摄像头帧，且该帧的获取时间和毫米波雷达帧获取时间相差不超过`0.3s`，则使用该摄像头帧更新形状，使用毫米波帧更新中心点；
      2. 如果超过`0.3s`就不进行更新；
   3. 如果最近的激光雷达帧和摄像头帧都不存在，才使用毫米波雷达帧更新形状和中心点；
3. 摄像头：
   1. 当不存在最近的激光雷达帧时使用摄像头进行更新。



#### 1.5 运动状态融合

鲁棒卡尔曼滤波




#### 1.-1 源码

文件框架：

```
── fusion (16879 lines)
   ├── app
   ├── base
   ├── common
   └── lib
       ├── data_association 
       │   └── hm_data_association		// 匈牙利匹配
       ├── data_fusion
       │   ├── existence_fusion
       │   │   └── dst_existence_fusion	// 存在性融合
       │   ├── motion_fusion
       │   │   └── kalman_motion_fusion	// 运动状态融合
       │   ├── shape_fusion
       │   │   └── pbf_shape_fusion		// 形状融合
       │   ├── tracker
       │   │   └── pbf_tracker			// 概率跟踪器（跟踪器入口）
       │   └── type_fusion
       │       └── dst_type_fusion		// 类型融合
       ├── dummy
       ├── fusion_system
       │   └── probabilistic_fusion		// 融合入口
       ├── gatekeeper
       │   └── pbf_gatekeeper			// 目标输出限制
       │       └── proto
       └── interface					// 接口
```

符号说明：

简单起见，下文中对C++代码中的变量类型采取简略写法，其对照表如下：

- vector\<T\> -> T[]
- vector\<vector\<T\>\> -> T[\][\]
- pair\<Tx, Ty\> -> {Tx, Ty}
- map\<Tx, Ty\> -> {Tx, Ty}[]

##### lib/data_fusion_existence_fusion

DS证据理论的实现，包含两个主要功能类`Dst`和`DstManager`，一个数据类`DstCommonData`。

- **DstCommonData**：Dst数据

| 成员变量                  | 含义                                                         | 初值  | 类型                     |
| ------------------------- | ------------------------------------------------------------ | ----- | ------------------------ |
| init_                     | 该数据是否被初始化                                           | false | bool                     |
| fod_loc_                  | 识别框架中全集的下标                                         | 0     | size\_t                  |
| fod_subsets_              | 识别框架，由bit位表示不同的元素，元素集合用元素之间与运算得到 |       | uint64_t[]               |
| fod_subset_cardinalities_ | 识别框架元素基数，表示当前元素是由几个假设复合得到           |       | size\_t[]                |
| fod_subset_names_         | 识别框架元素对应的字符串名称                                 |       | string[]                 |
| combination_relations_    | 存储识别框架中元素之间存在交集关系且交集是行下标对应的元素的元素对的邻接表，行下标表示元素A，行中存在一对下标{i,j}，表示以i为下标的元素B和以j为下标的元素C存在交集，且其交集为元素A，即$B \cap C =A$ |       | {size\_t, size\_t}[\][\] |
| subset_relations_         | 存储识别框架中元素之间的包含关系的邻接表，行下标表示元素A，行中存在下标i，表示以i为下标的元素B包含于A，即$B\subseteq A$ |       | size\_t[\][\]            |
| inter_relations_          | 存储识别框架中元素之间存在交集的邻接表，行下标表示元素A，行中存在下标i，表示以i为下标的元素B和A存在交集，即$B\cap A \neq \emptyset $ |       | size\_t[\][\]            |
| subsets_ind_map_          | 存储识别框架元素与其在识别框架中的下标编号的对应关系         |       | {uint64\_t, size\_t}[]   |

- **DstManager**：单例，管理实例化的Dst数据，并且通过提前计算加速证据理论的计算；

| 成员变量          | 含义        | 初值 | 类型                         |
| ----------------- | ----------- | ---- | ---------------------------- |
| dst\_common\_data | Dst数据存储 |      | map\<string, DstCommonData\> |

| 成员函数             | 含义                                                         | 参数                                                         | 返回值                                                  |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------- |
| AddApp               | 根据传入的App名称和识别框架初始化Dst数据                     | string app_name, uint64[] fod_subsets, string[] fod_subset_names | bool                                                    |
| IsAppAdded           | 判断当前App是否已被添加                                      | string app_name                                              | bool                                                    |
| GetAppDataPtr        | 根据App名称获取Dst数据                                       | string app_name                                              | DstCommonDataPtr                                        |
| FodSubsetToInd       | 根据App名称和识别框架元素，获取元素在识别框架中的下标        | string app_name, uint64 fod_subset                           | size_t                                                  |
| IndToFodSubset       | 根据App名称和下标，获取识别框架中对应的元素                  | string app_name, size_t ind                                  | uint64                                                  |
| BuildSubsetsIndMap   | 根据识别框架，建立识别框架中元素与其下标的对应关系           | DstCommonData* dst_data                                      | 无，结果存储在dst_data的subsets_ind_map_成员中          |
| FodCheck             | 检查识别框架中是否存在全集，如果没有则添加一个全集，避免Zadeh悖论 | DstCommonData* dst_data                                      | 无，结果用于修改dst_data的fod_subsets和fod_loc          |
| ComputeCardinalities | 计算识别框架中的每个元素由几个假设构成                       | DstCommonData* dst_data                                      | 无，结果存储在dst_data的成员fod_subset_cardinalities_中 |
| ComputeRelations\*   | 计算识别框架中所有元素之间的关系矩阵                         | DstCommonData* dst_data                                      | bool，关系计算存储在dst_data的三个关系矩阵成员中        |
| BuildNamesMap        | 设置识别框架元素的名称                                       | string[] fod_subset_names, DstCommonData* dst_data           | void，结果存储在dst_data的fod_subset_names_成员中       |

- **Dst**：DS证据理论实现，一个Dst的对象对应一个对识别框架的观测，主要是实现了证据合成公式、概率计算和置信度计算。

| 成员变量          | 含义                               | 初值 | 类型         |
| ----------------- | ---------------------------------- | ---- | ------------ |
| app_name_         | 所属DstApp的名称                   |      | string       |
| dst_data_ptr_     | 对应的Dst数据                      |      | DstCommonPtr |
| bba_vec_          | 识别框架的概率分配（BPA写成了bba） |      | double[]     |
| support_vec_      | 识别框架的信任函数Bel              |      | double[]     |
| plausibility_vec_ | 识别框架的似然函数Pl               |      | double[]     |
| uncertainty_vec_  | 识别框架的不确定度                 |      | double[]     |
| probability_vec_  | 各假设的概率分布                   |      | double[]     |

| 成员函数           | 含义                                                         | 参数                           | 返回值                                 |
| ------------------ | ------------------------------------------------------------ | ------------------------------ | -------------------------------------- |
| ComputeSptPlsUct   | 计算信任函数、似然函数以及不确定度                           |                                | 无，结果存储在三个vector中             |
| ComputeProbability | 根据BPA和假设基数计算概率                                    |                                | 无，概率存储在probability\_vec\_       |
| operator\+         | 根据公式进行证据合成                                         | const Dst &lhs, const Dst &rhs | Dst                                    |
| operator\*         | 实现证据加权，对BPA中的每一个证据乘以w，将fod\_loc\_位置的证据设置为$1-w+w\times bpa[fod\_loc\_]$ | const Dst &dst, double w       | Dst                                    |
| Normalize          | 归一化概率分配bba\_vec\_中的元素                             |                                | 修改bba\_vec\_，使其所有元素的和为1    |
| SelfCheck          | 检查dst数据指针，如为空，重新获取并重置BPA                   |                                | 无，修改dst\_data\_ptr\_以及bba\_vec\_ |

概率计算公式：（不是很理解，求得的概率之和不为1）

$$
P(B)= \sum \limits_{B \cap C = A} \frac{Cardinality_A \times m(C)}{Cardinality_C}
$$



##### lib/fusion_system

概率融合入口，包含了融合的主要流程。

接收到传感器帧时调用



##### lib/interface

定义了融合组件中使用的接口

**base_data_association.h**

匹配算法接口，定义了以下几个类：

| 类名                | 含义         | 备注           |
| ------------------- | ------------ | -------------- |
| AssociationOptions  | 匹配选项     |                |
| AssociationResult   | 匹配结果存储 |                |
| BaseDataAssociation | 匹配算法基类 | 不可拷贝、移动 |

**base_existence_fusion.h**

存在性融合算法接口

| 类名                | 含义               | 备注           |
| ------------------- | ------------------ | -------------- |
| BaseExistenceFusion | 存在性融合算法基类 | 不可拷贝、移动 |

**base_fusion_system.h**

融合系统接口。融合系统用于对传感器数据帧进行融合。

| 类名              | 含义           | 备注           |
| ----------------- | -------------- | -------------- |
| FusionInitOptions | 融合初始化选项 |                |
| FusionOptions     | 融合选项       |                |
| BaseFusionSystem  | 融合系统基类   | 不可拷贝、移动 |

**base_gatekeeper.h**

门限接口。门限通过一些条件，判断目标是否能够被`publish`。

| 类名           | 含义     | 备注           |
| -------------- | -------- | -------------- |
| BaseGatekeeper | 门限基类 | 不可拷贝、移动 |

**base_motion_fusion.h**

运动融合接口。

| 类名             | 含义         | 备注           |
| ---------------- | ------------ | -------------- |
| BaseMotionFusion | 运动融合基类 | 不可拷贝、移动 |

**base_multisensor_fusion.h**

多传感器融合接口

| 类名                           | 含义             | 备注           |
| ------------------------------ | ---------------- | -------------- |
| ObstacleMultiSensorFusionParam | 融合参数         |                |
| BaseMultiSensorFusion          | 多传感器融合基类 | 不可拷贝、移动 |

**base_shape_fusion.h**

形状融合接口

| 类名            | 含义         | 备注           |
| --------------- | ------------ | -------------- |
| BaseShapeFusion | 形状融合基类 | 不可拷贝、移动 |

**base_tracker.h**

跟踪器接口

| 类名           | 含义       | 备注           |
| -------------- | ---------- | -------------- |
| TrackerOptions | 跟踪器选项 |                |
| BaseTracker    | 跟踪器基类 | 不可拷贝、移动 |

**base_type_fusion.h**

类型融合接口

| 类名           | 含义         | 备注           |
| -------------- | ------------ | -------------- |
| BaseTypeFusion | 类型融合基类 | 不可拷贝、移动 |



##### app

**obstacle_multi_sensor_fusion.h**



### Refs

1. [CyberRT框架速读](https://blog.csdn.net/briblue/article/details/123432580)
2. [CyberRT和ros对照](https://blog.csdn.net/kesalin/article/details/88914029)
2. [DS证据理论 浙大课程ppt](https://wenku.baidu.com/view/8da2a02d011ca300a6c390d3.html)
2. [知乎用户GoodluckTian对Apollo中证据理论的分析](https://zhuanlan.zhihu.com/p/389572333)



### Addition

1. 目标定义：`modules/perception/base/object.h`

基础目标：

```c++
struct alignas(16) Object {
  EIGEN_MAKE_ALIGNED_OPERATOR_NEW

  Object();
  std::string ToString() const;
  void Reset();

  // @brief object id per frame, required
  int id = -1;

  // @brief convex hull of the object, required
  PointCloud<PointD> polygon;

  // oriented boundingbox information
  // @brief main direction of the object, required
  Eigen::Vector3f direction = Eigen::Vector3f(1, 0, 0);
  /*@brief the yaw angle, theta = 0.0 <=> direction(1, 0, 0),
    currently roll and pitch are not considered,
    make sure direction and theta are consistent, required
  */
  float theta = 0.0f;
  // @brief theta variance, required
  float theta_variance = 0.0f;
  // @brief center of the boundingbox (cx, cy, cz), required
  Eigen::Vector3d center = Eigen::Vector3d(0, 0, 0);
  // @brief covariance matrix of the center uncertainty, required
  Eigen::Matrix3f center_uncertainty;
  /* @brief size = [length, width, height] of boundingbox
     length is the size of the main direction, required
  */
  Eigen::Vector3f size = Eigen::Vector3f(0, 0, 0);
  // @brief size variance, required
  Eigen::Vector3f size_variance = Eigen::Vector3f(0, 0, 0);
  // @brief anchor point, required
  Eigen::Vector3d anchor_point = Eigen::Vector3d(0, 0, 0);

  // @brief object type, required
  ObjectType type = ObjectType::UNKNOWN;
  // @brief probability for each type, required
  std::vector<float> type_probs;

  // @brief object sub-type, optional
  ObjectSubType sub_type = ObjectSubType::UNKNOWN;
  // @brief probability for each sub-type, optional
  std::vector<float> sub_type_probs;

  // @brief existence confidence, required
  float confidence = 1.0f;

  // tracking information
  // @brief track id, required
  int track_id = -1;
  // @brief velocity of the object, required
  Eigen::Vector3f velocity = Eigen::Vector3f(0, 0, 0);
  // @brief covariance matrix of the velocity uncertainty, required
  Eigen::Matrix3f velocity_uncertainty;
  // @brief if the velocity estimation is converged, true by default
  bool velocity_converged = true;
  // @brief velocity confidence, required
  float velocity_confidence = 1.0f;
  // @brief acceleration of the object, required
  Eigen::Vector3f acceleration = Eigen::Vector3f(0, 0, 0);
  // @brief covariance matrix of the acceleration uncertainty, required
  Eigen::Matrix3f acceleration_uncertainty;

  // @brief age of the tracked object, required
  double tracking_time = 0.0;
  // @brief timestamp of latest measurement, required
  double latest_tracked_time = 0.0;

  // @brief motion state of the tracked object, required
  MotionState motion_state = MotionState::UNKNOWN;
  // // Tailgating (trajectory of objects)
  std::array<Eigen::Vector3d, 100> drops;
  std::size_t drop_num = 0;
  // // CIPV
  bool b_cipv = false;
  // @brief brake light, left-turn light and right-turn light score, optional
  CarLight car_light;
  // @brief sensor-specific object supplements, optional
  LidarObjectSupplement lidar_supplement;
  RadarObjectSupplement radar_supplement;
  CameraObjectSupplement camera_supplement;
  FusionObjectSupplement fusion_supplement;

  // @debug feature to be used for semantic mapping
//  std::shared_ptr<apollo::prediction::Feature> feature;
};
```

传感器目标：

```c++
class SensorObject {
 public:
  SensorObject() = delete;

  explicit SensorObject(const std::shared_ptr<const base::Object>& object_ptr);

  SensorObject(const std::shared_ptr<const base::Object>& object_ptr,
               const std::shared_ptr<const SensorFrameHeader>& frame_header);

  SensorObject(const std::shared_ptr<const base::Object>& object_ptr,
               const std::shared_ptr<SensorFrame>& frame_ptr);

  // Getter
  // @brief get frame timestamp which might be different with object timestamp
  double GetTimestamp() const;
  bool GetRelatedFramePose(Eigen::Affine3d* pose) const;

  std::string GetSensorId() const;
  base::SensorType GetSensorType() const;

  inline std::shared_ptr<const base::Object> GetBaseObject() const {
    return object_;
  }

  inline double GetInvisiblePeriod() const { return invisible_period_; }

  inline void SetInvisiblePeriod(double period) { invisible_period_ = period; }

 private:
  FRIEND_TEST(SensorObjectTest, test);

  std::shared_ptr<const base::Object> object_;	// 存储目标
  double invisible_period_ = 0.0;				// 目标从所有传感器中消失的时间
  std::shared_ptr<const SensorFrameHeader> frame_header_ = nullptr;
};

typedef std::shared_ptr<SensorObject> SensorObjectPtr;
typedef std::shared_ptr<const SensorObject> SensorObjectConstPtr;
```



融合目标：就多封装了获取时间戳的成员函数

```c++

class FusedObject {
 public:
  FusedObject();
  ~FusedObject() = default;

  inline double GetTimestamp() const { return object_->latest_tracked_time; }

  inline std::shared_ptr<base::Object> GetBaseObject() { return object_; }

 private:
  std::shared_ptr<base::Object> object_;
};

typedef std::shared_ptr<FusedObject> FusedObjectPtr;
```





2. 感知阶段：

   `modules/perception/onboard/inner_component_messages/inner_component_messages.h`

```c++
enum class ProcessStage {
  LIDAR_PREPROCESS = 0,
  LIDAR_SEGMENTATION = 1,
  LIDAR_RECOGNITION = 2,
  STEREO_CAMERA_DETECTION = 3,		// 立体相机（双目）
  MONOCULAR_CAMERA_DETECTION = 4,	// 单目相机
  LONG_RANGE_RADAR_DETECTION = 5,
  SHORT_RANGE_RADAR_DETECTION = 6,
  ULTRASONIC_DETECTION = 7,			// 超声波
  SENSOR_FUSION = 8,
  UNKNOWN_STAGE = 9,
  PROCESSSTAGE_COUNT = 10,
  LIDAR_DETECTION = 11
};
```

3. 感知帧消息：

   `modules/perception/onboard/inner_component_messages/inner_component_messages.h`

```c++
class SensorFrameMessage {
 public:
  SensorFrameMessage() { type_name_ = "SensorFrameMessage"; }
  ~SensorFrameMessage() = default;
  std::string GetTypeName() { return type_name_; }
  SensorFrameMessage* New() const { return new SensorFrameMessage; }

 public:
  apollo::common::ErrorCode error_code_ = apollo::common::ErrorCode::OK;

  std::string sensor_id_;
  double timestamp_ = 0.0;
  uint64_t lidar_timestamp_ = 0;
  uint32_t seq_num_ = 0;
  std::string type_name_;
  base::HdmapStructConstPtr hdmap_;

  base::FramePtr frame_;	// 存储感知帧

  ProcessStage process_stage_ = ProcessStage::UNKNOWN_STAGE;
};
```

4. 帧：

   `modules/perception/base/frame.h`

```c++
struct alignas(16) Frame {
  EIGEN_MAKE_ALIGNED_OPERATOR_NEW

  Frame() { sensor2world_pose.setIdentity(); }

  void Reset() {
    timestamp = 0.0;
    objects.clear();
    sensor2world_pose.setIdentity();
    sensor_info.Reset();
    lidar_frame_supplement.Reset();
    radar_frame_supplement.Reset();
    camera_frame_supplement.Reset();
  }
  // @brief sensor information
  SensorInfo sensor_info;

  double timestamp = 0.0;
  std::vector<std::shared_ptr<Object>> objects;	// 目标列表
  Eigen::Affine3d sensor2world_pose;			// 传感器坐标世界坐标仿射变换矩阵

  // sensor-specific frame supplements
  LidarFrameSupplement lidar_frame_supplement;
  RadarFrameSupplement radar_frame_supplement;
  CameraFrameSupplement camera_frame_supplement;
  UltrasonicFrameSupplement ultrasonic_frame_supplement;
};

typedef std::shared_ptr<Frame> FramePtr;
typedef std::shared_ptr<const Frame> FrameConstPtr;
```

5. 场景

```c++
class Scene {
 public:
  Scene();
  ~Scene();

  inline std::vector<TrackPtr>& GetForegroundTracks() {
    return foreground_tracks_;
  }

  inline const std::vector<TrackPtr>& GetForegroundTracks() const {
    return foreground_tracks_;
  }

  inline std::vector<TrackPtr>& GetBackgroundTracks() {
    return background_tracks_;
  }

  inline const std::vector<TrackPtr>& GetBackgroundTracks() const {
    return background_tracks_;
  }

  void AddForegroundTrack(TrackPtr track);
  void AddBackgroundTrack(TrackPtr track);

 protected:
  std::vector<TrackPtr> foreground_tracks_;
  std::vector<TrackPtr> background_tracks_;
};

typedef std::shared_ptr<Scene> ScenePtr;
typedef std::shared_ptr<const Scene> SceneConstPtr;
```

6. 感知帧

```c++
class SensorFrame {
 public:
  SensorFrame();

  explicit SensorFrame(const base::FrameConstPtr& base_frame_ptr);

  void Initialize(const base::FrameConstPtr& base_frame_ptr);

  void Initialize(const base::FrameConstPtr& base_frame_ptr,
                  const SensorPtr& sensor);

  // Getter
  inline double GetTimestamp() const { return header_->timestamp; }

  inline bool GetPose(Eigen::Affine3d* pose) const {
    if (pose == nullptr) {
      AERROR << "pose is not available";
      return false;
    }
    *pose = header_->sensor2world_pose;
    return true;
  }

  inline std::vector<SensorObjectPtr>& GetForegroundObjects() {
    return foreground_objects_;
  }

  inline const std::vector<SensorObjectPtr>& GetForegroundObjects() const {
    return foreground_objects_;
  }

  inline std::vector<SensorObjectPtr>& GetBackgroundObjects() {
    return background_objects_;
  }

  inline const std::vector<SensorObjectPtr>& GetBackgroundObjects() const {
    return background_objects_;
  }

  std::string GetSensorId() const;

  base::SensorType GetSensorType() const;

  SensorFrameHeaderConstPtr GetHeader() const { return header_; }

 private:
  std::vector<SensorObjectPtr> foreground_objects_;
  std::vector<SensorObjectPtr> background_objects_;

  // sensor-specific frame supplements
  base::LidarFrameSupplement lidar_frame_supplement_;
  base::RadarFrameSupplement radar_frame_supplement_;
  base::CameraFrameSupplement camera_frame_supplement_;

  SensorFrameHeaderPtr header_ = nullptr;
};
```

7. 轨迹，在目标属性的基础上增加了一些融合使用的属性

```c++
// modules/perception/fusion/base/track.h
class Track {
 public:
  Track();
  virtual ~Track() = default;

  Track(const Track&) = delete;
  Track& operator=(const Track&) = delete;

  // static members initialization
  inline static void SetMaxLidarInvisiblePeriod(double period) {
    s_max_lidar_invisible_period_ = period;
  }
  inline static void SetMaxRadarInvisiblePeriod(double period) {
    s_max_radar_invisible_period_ = period;
  }
  inline static void SetMaxCameraInvisiblePeriod(double period) {
    s_max_camera_invisible_period_ = period;
  }

  bool Initialize(SensorObjectPtr obj, bool is_background = false);

  void Reset();

  SensorObjectConstPtr GetSensorObject(const std::string& sensor_id) const;
  SensorObjectConstPtr GetLatestLidarObject() const;
  SensorObjectConstPtr GetLatestRadarObject() const;
  SensorObjectConstPtr GetLatestCameraObject() const;

  inline FusedObjectPtr GetFusedObject() { return fused_object_; }
  inline SensorId2ObjectMap& GetLidarObjects() { return lidar_objects_; }

  inline const SensorId2ObjectMap& GetLidarObjects() const {
    return lidar_objects_;
  }

  inline SensorId2ObjectMap& GetRadarObjects() { return radar_objects_; }

  inline const SensorId2ObjectMap& GetRadarObjects() const {
    return radar_objects_;
  }

  inline SensorId2ObjectMap& GetCameraObjects() { return camera_objects_; }

  inline const SensorId2ObjectMap& GetCameraObjects() const {
    return camera_objects_;
  }

  inline int GetTrackId() const {
    return fused_object_->GetBaseObject()->track_id;
  }

  inline double GetTrackingPeriod() const { return tracking_period_; }

  inline size_t GetTrackedTimes() const { return tracked_times_; }

  inline void AddTrackedTimes() { ++tracked_times_; }

  inline double GetExistenceProb() const { return existence_prob_; }

  inline void SetExistenceProb(double prob) { existence_prob_ = prob; }

  inline double GetToicProb() const { return toic_prob_; }

  inline void SetToicProb(double prob) { toic_prob_ = prob; }
  inline bool IsBackground() const { return is_background_; }

  inline bool IsAlive() const { return is_alive_; }

  bool IsVisible(const std::string& sensor_id) const;
  bool IsLidarVisible() const;
  bool IsRadarVisible() const;
  bool IsCameraVisible() const;

  static size_t GenerateNewTrackId();

  void UpdateWithSensorObject(const SensorObjectPtr& obj);

  void UpdateWithoutSensorObject(const std::string& sensor_id,
                                 double measurement_timestamp);

  std::string DebugString() const;

 protected:
  // update state
  void UpdateSupplementState(const SensorObjectPtr& src_object = nullptr);
  void UpdateUnfusedState(const SensorObjectPtr& src_object);

  SensorObjectConstPtr GetLatestSensorObject(
      const SensorId2ObjectMap& objects) const;
  void UpdateSensorObject(SensorId2ObjectMap* objects,
                          const SensorObjectPtr& obj);
  void UpdateSensorObjectWithoutMeasurement(SensorId2ObjectMap* objects,
                                            const std::string& sensor_id,
                                            double measurement_timestamp,
                                            double max_invisible_period);
  void UpdateSensorObjectWithMeasurement(SensorId2ObjectMap* objects,
                                         const std::string& sensor_id,
                                         double measurement_timestamp,
                                         double max_invisible_period);
  void UpdateWithSensorObjectForBackground(const SensorObjectPtr& obj);
  void UpdateWithoutSensorObjectForBackground(const std::string& sensor_id,
                                              double measurement_timestamp);

 protected:
  SensorId2ObjectMap lidar_objects_;
  SensorId2ObjectMap radar_objects_;
  SensorId2ObjectMap camera_objects_;

  FusedObjectPtr fused_object_ = nullptr;
  double tracking_period_ = 0.0;
  double existence_prob_ = 0.0;	// 存在概率
  double toic_prob_ = 0.0;		// 在摄像头视野的概率

  bool is_background_ = false;	// 是否为后景目标
  bool is_alive_ = true;

  size_t tracked_times_ = 0;

 private:
  FRIEND_TEST(TrackTest, test);

  static size_t s_track_idx_;
  static double s_max_lidar_invisible_period_;
  static double s_max_radar_invisible_period_;
  static double s_max_camera_invisible_period_;
};

typedef std::shared_ptr<Track> TrackPtr;
typedef std::shared_ptr<const Track> TrackConstPtr;
```

8. 概率跟踪器

```c++
//modules/perception/fusion/lib/data_fusion/tracker/pbf_tracker/pbf_tracker.h
class PbfTracker : public BaseTracker {
 public:
  PbfTracker();
  virtual ~PbfTracker();

  PbfTracker(const PbfTracker&) = delete;
  PbfTracker& operator=(const PbfTracker&) = delete;

  static bool InitParams();

  bool Init(TrackPtr track, SensorObjectPtr measurement) override;

  void UpdateWithMeasurement(const TrackerOptions& options,
                             const SensorObjectPtr measurement,
                             double target_timestamp) override;

  void UpdateWithoutMeasurement(const TrackerOptions& options,
                                const std::string& sensor_id,
                                double measurement_timestamp,
                                double target_timestamp) override;

  std::string Name() const override;

 protected:
  bool InitMethods();

 protected:
  static std::string s_type_fusion_method_;
  static std::string s_motion_fusion_method_;
  static std::string s_shape_fusion_method_;
  static std::string s_existence_fusion_method_;

  std::unique_ptr<BaseTypeFusion> type_fusion_ = nullptr;
  std::unique_ptr<BaseMotionFusion> motion_fusion_ = nullptr;
  std::unique_ptr<BaseExistenceFusion> existence_fusion_ = nullptr;
  std::unique_ptr<BaseShapeFusion> shape_fusion_ = nullptr;
};
```





